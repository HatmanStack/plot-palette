name: Performance Tests

on:
  workflow_dispatch:  # Manual trigger only
    inputs:
      environment:
        description: 'Environment to test'
        required: true
        default: 'staging'
        type: choice
        options:
          - staging
          - production
      users:
        description: 'Number of concurrent users'
        required: true
        default: '50'
      duration:
        description: 'Test duration (e.g., 5m, 1h)'
        required: true
        default: '5m'

jobs:
  load-test:
    name: Locust Load Test
    runs-on: ubuntu-latest
    timeout-minutes: 120

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install Locust
        run: |
          python -m pip install --upgrade pip
          pip install locust

      - name: Get API endpoint
        id: get-endpoint
        run: |
          ENV="${{ github.event.inputs.environment }}"
          STACK_NAME="plot-palette-${ENV}"

          API_ENDPOINT=$(aws cloudformation describe-stacks \
            --stack-name ${STACK_NAME} \
            --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' \
            --output text)

          echo "api-endpoint=${API_ENDPOINT}" >> $GITHUB_OUTPUT
          echo "Testing endpoint: ${API_ENDPOINT}"
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Run Locust load test
        env:
          API_ENDPOINT: ${{ steps.get-endpoint.outputs.api-endpoint }}
        run: |
          locust -f tests/performance/locustfile.py \
            --host=$API_ENDPOINT \
            --headless \
            --users=${{ github.event.inputs.users }} \
            --spawn-rate=5 \
            --run-time=${{ github.event.inputs.duration }} \
            --html=performance-report.html \
            --csv=performance-results

      - name: Upload performance report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-report
          path: |
            performance-report.html
            performance-results*.csv
          retention-days: 30

      - name: Check performance thresholds
        run: |
          # Check if p95 response time is under 500ms
          P95_TIME=$(grep "Aggregated" performance-results_stats.csv | awk -F',' '{print $9}')

          echo "P95 Response Time: ${P95_TIME}ms"

          if [ $(echo "$P95_TIME > 500" | bc) -eq 1 ]; then
            echo "❌ Performance test failed: P95 response time (${P95_TIME}ms) exceeds 500ms threshold"
            exit 1
          fi

          echo "✅ Performance test passed: P95 response time is ${P95_TIME}ms"

  stress-test:
    name: Stress Test
    runs-on: ubuntu-latest
    needs: load-test
    if: success()

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.13
        uses: actions/setup-python@v5
        with:
          python-version: '3.13'

      - name: Install Locust
        run: |
          python -m pip install --upgrade pip
          pip install locust

      - name: Get API endpoint
        id: get-endpoint
        run: |
          ENV="${{ github.event.inputs.environment }}"
          STACK_NAME="plot-palette-${ENV}"

          API_ENDPOINT=$(aws cloudformation describe-stacks \
            --stack-name ${STACK_NAME} \
            --query 'Stacks[0].Outputs[?OutputKey==`ApiEndpoint`].OutputValue' \
            --output text)

          echo "api-endpoint=${API_ENDPOINT}" >> $GITHUB_OUTPUT
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION || 'us-east-1' }}

      - name: Run stress test
        env:
          API_ENDPOINT: ${{ steps.get-endpoint.outputs.api-endpoint }}
        run: |
          # Stress test with 200 users
          locust -f tests/performance/locustfile.py \
            --host=$API_ENDPOINT \
            --headless \
            --users=200 \
            --spawn-rate=20 \
            --run-time=5m \
            --html=stress-test-report.html \
            --csv=stress-test-results

      - name: Upload stress test report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: stress-test-report
          path: |
            stress-test-report.html
            stress-test-results*.csv
          retention-days: 30

  notify-results:
    name: Notify Performance Results
    runs-on: ubuntu-latest
    needs: [load-test, stress-test]
    if: always()

    steps:
      - name: Performance test summary
        run: |
          echo "## Performance Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "Environment: ${{ github.event.inputs.environment }}" >> $GITHUB_STEP_SUMMARY
          echo "Users: ${{ github.event.inputs.users }}" >> $GITHUB_STEP_SUMMARY
          echo "Duration: ${{ github.event.inputs.duration }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          if [ "${{ needs.load-test.result }}" == "success" ]; then
            echo "✅ Load test passed" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Load test failed" >> $GITHUB_STEP_SUMMARY
          fi

          if [ "${{ needs.stress-test.result }}" == "success" ]; then
            echo "✅ Stress test passed" >> $GITHUB_STEP_SUMMARY
          elif [ "${{ needs.stress-test.result }}" == "skipped" ]; then
            echo "⏭️ Stress test skipped" >> $GITHUB_STEP_SUMMARY
          else
            echo "❌ Stress test failed" >> $GITHUB_STEP_SUMMARY
          fi
